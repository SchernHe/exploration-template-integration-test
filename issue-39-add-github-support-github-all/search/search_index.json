{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Integration Test Instance","text":"<p>Lorem ipsum...</p> <p>Project based on Explora - The Machine Learning Exploration Template</p>"},{"location":"reference/","title":"Reference","text":""},{"location":"reference/#integration_test_instance","title":"<code>integration_test_instance</code>","text":""},{"location":"reference/#integration_test_instance.constants","title":"<code>constants</code>","text":""},{"location":"reference/#integration_test_instance.constants.DATA_ROOT_DIR","title":"<code>DATA_ROOT_DIR = PROJECT_ROOT / 'data'</code>  <code>module-attribute</code>","text":"<p>Parent directory for all data artifacts</p>"},{"location":"reference/#integration_test_instance.constants.PROJECT_ROOT","title":"<code>PROJECT_ROOT = Path(__file__).parents[2]</code>  <code>module-attribute</code>","text":"<p>Root path of this project</p>"},{"location":"reference/#integration_test_instance.constants.RAW_DATA_FILE","title":"<code>RAW_DATA_FILE = DATA_ROOT_DIR / 'raw/AmesHousing.csv'</code>  <code>module-attribute</code>","text":"<p>Path to the raw data file</p>"},{"location":"reference/#integration_test_instance.io","title":"<code>io</code>","text":""},{"location":"reference/#integration_test_instance.io.read_raw_data","title":"<code>read_raw_data(p: Path) -&gt; pd.DataFrame</code>","text":"<p>Preconfigured way to open our raw data. As soon as custom logic is necessary to open data (in this case: <code>index_col</code>), put it in its own function.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Path</code> <pre><code>Path to the file.\n</code></pre> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with \"Order\" as the index.</p> Source code in <code>src/integration_test_instance/io/__init__.py</code> <pre><code>def read_raw_data(p: Path) -&gt; pd.DataFrame:\n    \"\"\"Preconfigured way to open our raw data.\n    As soon as custom logic is necessary to open data (in this case: `index_col`), put\n    it in its own function.\n\n    Parameters\n    ----------\n    p : Path\n            Path to the file.\n\n    Returns\n    -------\n    pd.DataFrame\n            Dataframe with \"Order\" as the index.\n    \"\"\"\n    df = pd.read_csv(p, index_col=\"Order\", sep=\"\\t\")\n\n    return df\n</code></pre>"},{"location":"reference/#integration_test_instance.io.read_raw_data_with_schema","title":"<code>read_raw_data_with_schema(p: Path) -&gt; pandera.typing.DataFrame[RawDataSchema]</code>","text":"<p>Alternative function to <code>read_raw_data</code>. Reads raw data from the path and checks whether the RawDataSchema is followed.</p> <p>Parameters:</p> Name Type Description Default <code>p</code> <code>Path</code> <pre><code>Path to the raw data file\n</code></pre> required <p>Returns:</p> Type Description <code>DataFrame[RawDataSchema]</code> <p>Dataframe with the schema checked</p> Source code in <code>src/integration_test_instance/io/__init__.py</code> <pre><code>@pandera.check_types(lazy=True)\ndef read_raw_data_with_schema(p: Path) -&gt; pandera.typing.DataFrame[RawDataSchema]:\n    \"\"\"Alternative function to `read_raw_data`.\n    Reads raw data from the path and checks whether the\n    [RawDataSchema][integration_test_instance.io.schemas.RawDataSchema] is followed.\n\n    Parameters\n    ----------\n    p : Path\n            Path to the raw data file\n\n    Returns\n    -------\n    pandera.typing.DataFrame[RawDataSchema]\n            Dataframe with the schema checked\n    \"\"\"\n    raw_data = read_raw_data(p)\n\n    raw_pandera_data = pandera.typing.DataFrame[RawDataSchema](raw_data)\n\n    return raw_pandera_data\n</code></pre>"},{"location":"reference/#integration_test_instance.io.schemas","title":"<code>schemas</code>","text":""},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema","title":"<code>RawDataSchema</code>","text":"<p>             Bases: <code>SchemaModel</code></p> <p>InputSchema, used in RAW_DATA_FILE</p> Source code in <code>src/integration_test_instance/io/schemas.py</code> <pre><code>class RawDataSchema(pa.SchemaModel):\n    \"\"\"InputSchema, used in [RAW_DATA_FILE][integration_test_instance.constants.RAW_DATA_FILE]\"\"\"\n\n    order: Index[int] = pa.Field(alias=\"Order\", ge=0, check_name=True, unique=True)\n    \"\"\"Observation number.\"\"\"\n    pid: Series[int] = pa.Field(alias=\"PID\", unique=True, gt=0)\n    \"\"\"Parcel identification number - can be used with city web site for parcel review.\"\"\"\n    sale_price: Series[int] = pa.Field(alias=\"SalePrice\", gt=0)\n    \"\"\"The sale price of the property\"\"\"\n    street: Series[str] = pa.Field(alias=\"Street\", isin=[\"Pave\", \"Grvl\"])\n    \"\"\"Type of road access to property. Either `Pave` or `Grvl`\"\"\"\n    fence: Series[str] = pa.Field(alias=\"Fence\", nullable=True)\n    \"\"\"Fence quality, can be `NaN`\"\"\"\n</code></pre>"},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema.fence","title":"<code>fence: Series[str] = pa.Field(alias='Fence', nullable=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Fence quality, can be <code>NaN</code></p>"},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema.order","title":"<code>order: Index[int] = pa.Field(alias='Order', ge=0, check_name=True, unique=True)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Observation number.</p>"},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema.pid","title":"<code>pid: Series[int] = pa.Field(alias='PID', unique=True, gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Parcel identification number - can be used with city web site for parcel review.</p>"},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema.sale_price","title":"<code>sale_price: Series[int] = pa.Field(alias='SalePrice', gt=0)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The sale price of the property</p>"},{"location":"reference/#integration_test_instance.io.schemas.RawDataSchema.street","title":"<code>street: Series[str] = pa.Field(alias='Street', isin=['Pave', 'Grvl'])</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Type of road access to property. Either <code>Pave</code> or <code>Grvl</code></p>"},{"location":"reference/#integration_test_instance.mlops","title":"<code>mlops</code>","text":""},{"location":"reference/#integration_test_instance.mlops.jupytext_to_executed_notebook","title":"<code>jupytext_to_executed_notebook</code>","text":""},{"location":"reference/#integration_test_instance.mlops.jupytext_to_executed_notebook.jupytext_to_executed_notebook","title":"<code>jupytext_to_executed_notebook(jupytext_path: os.PathLike, executed_notebook_path: os.PathLike) -&gt; None</code>","text":"<p>Reads a jupytext file, executes it, and stores the notebook with results as <code>.ipynb</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>jupytext_path</code> <code>PathLike</code> <p>Path to a jupytext File</p> required <code>executed_notebook_path</code> <code>PathLike</code> <p>Path where the generated notebook is supposed to be stored, stored as <code>.ipynb</code>.</p> required Source code in <code>src/integration_test_instance/mlops/jupytext_to_executed_notebook.py</code> <pre><code>def jupytext_to_executed_notebook(\n    jupytext_path: os.PathLike, executed_notebook_path: os.PathLike\n) -&gt; None:\n    \"\"\"Reads a [jupytext](https://jupytext.readthedocs.io/en/latest/) file, executes it,\n    and stores the notebook with results as `.ipynb` file.\n\n    Parameters\n    ----------\n    jupytext_path : os.PathLike\n        Path to a jupytext File\n    executed_notebook_path : os.PathLike\n        Path where the generated notebook is supposed to be stored, stored as `.ipynb`.\n    \"\"\"\n    jupytext_path = Path(jupytext_path)\n    executed_notebook_path = Path(executed_notebook_path)\n\n    jupytext_code = jupytext.read(fp=str(jupytext_path), fmt=\"py:percent\")\n    ExecutePreprocessor().preprocess(jupytext_code)\n\n    executed_notebook_path.parent.mkdir(exist_ok=True, parents=True)\n\n    nbformat.write(\n        nb=jupytext_code, fp=str(executed_notebook_path.with_suffix(\".ipynb\"))\n    )\n</code></pre>"},{"location":"reference/#integration_test_instance.mlops.jupytext_to_executed_notebook.main","title":"<code>main() -&gt; None</code>","text":"<p>Reads <code>sys.argv</code> for a path to a file or directory containing <code>.py</code> files with jupytext, executes them, and stores the results as <code>.ipynb</code> in the provided output file or directory.</p> <p>Usage: python jupytext_to_executed_notebook.py in_file_or_dir out_file_or_dir</p> Source code in <code>src/integration_test_instance/mlops/jupytext_to_executed_notebook.py</code> <pre><code>def main() -&gt; None:\n    \"\"\"Reads `sys.argv` for a path to a file or directory containing `.py` files with\n    [jupytext](https://jupytext.readthedocs.io/en/latest/), executes them, and stores\n    the results as `.ipynb` in the provided output file or directory.\n\n    Usage: python jupytext_to_executed_notebook.py in_file_or_dir out_file_or_dir\n    \"\"\"\n    if len(sys.argv) &lt; 3:\n        logger.error(\"Not enough arguments provided\")\n        logger.info(\n            \"Usage: python jupytext_to_executed_notebook.py in_file_or_dir out_file_or_dir\"\n        )\n        sys.exit(1)\n\n    in_path = Path(sys.argv[1])\n    out_path = Path(sys.argv[2])\n\n    in_files = []\n    if in_path.is_dir():\n        in_files = list(sorted(in_path.glob(\"**/*.py\")))\n    else:\n        in_files = [in_path]\n\n    for f in in_files:\n        if in_path.is_dir():\n            relative_in_file = f.relative_to(in_path)\n            out_file = out_path / relative_in_file.with_suffix(\".ipynb\")\n            out_file.parent.mkdir(parents=True, exist_ok=True)\n        else:\n            out_file = out_path.with_suffix(\".ipynb\")\n            out_file.parent.mkdir(parents=True, exist_ok=True)\n\n        logger.info(f\"Executing Jupytext {f}, storing executed notebook in {out_file}\")\n\n        start_time = datetime.datetime.now()\n        jupytext_to_executed_notebook(f, out_file)\n        end_time = datetime.datetime.now()\n\n        logger.debug(f\"Executing {f} took {end_time - start_time}\")\n</code></pre>"},{"location":"technical_details/","title":"Technical Details","text":""},{"location":"technical_details/#toolstack","title":"Toolstack","text":"<ul> <li>VSCode Devcontainer</li> <li>Git LFS for networked Data storage &amp; versioning</li> <li>Pre-commit setup</li> </ul>"},{"location":"technical_details/#ci-setup","title":"CI Setup","text":"<ul> <li>In your project create a cleanup policy, 7 days should be enough<ul> <li>docs</li> </ul> </li> <li>Create an project access token and add the token to the GitLab CI variables<ul> <li>docs</li> </ul> </li> </ul>"},{"location":"technical_details/#documentation","title":"Documentation","text":"<ul> <li>Created with mkdocs and mike.</li> <li>Uses mkdocs-nav-weight to order the documentation</li> </ul>"},{"location":"reports/01_exploration/","title":"1) Raw Data Exploration","text":"In\u00a0[1]: Copied! <pre>from IPython import get_ipython\n\nif ipython := get_ipython():\n    # If we're in an ipython environment, set the magic\n    ipython.run_line_magic(\"load_ext\", line=\"autoreload\")\n    ipython.run_line_magic(\"autoreload\", line=\"all\")\n</pre> from IPython import get_ipython  if ipython := get_ipython():     # If we're in an ipython environment, set the magic     ipython.run_line_magic(\"load_ext\", line=\"autoreload\")     ipython.run_line_magic(\"autoreload\", line=\"all\") In\u00a0[2]: Copied! <pre>from integration_test_instance.constants import PROJECT_ROOT\n\nprint(PROJECT_ROOT)\n</pre> from integration_test_instance.constants import PROJECT_ROOT  print(PROJECT_ROOT) <pre>/__w/exploration-template-integration-test/exploration-template-integration-test\n</pre> In\u00a0[3]: Copied! <pre>from integration_test_instance import constants, io\n</pre> from integration_test_instance import constants, io In\u00a0[4]: Copied! <pre>constants.RAW_DATA_FILE\n</pre> constants.RAW_DATA_FILE Out[4]: <pre>PosixPath('/__w/exploration-template-integration-test/exploration-template-integration-test/data/raw/AmesHousing.csv')</pre> In\u00a0[5]: Copied! <pre>raw_df = io.read_raw_data(constants.RAW_DATA_FILE)\n</pre> raw_df = io.read_raw_data(constants.RAW_DATA_FILE) In\u00a0[6]: Copied! <pre>raw_df.head()\n</pre> raw_df.head() Out[6]: PID MS SubClass MS Zoning Lot Frontage Lot Area Street Alley Lot Shape Land Contour Utilities ... Pool Area Pool QC Fence Misc Feature Misc Val Mo Sold Yr Sold Sale Type Sale Condition SalePrice Order 1 526301100 20 RL 141.0 31770 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 5 2010 WD Normal 215000 2 526350040 20 RH 80.0 11622 Pave NaN Reg Lvl AllPub ... 0 NaN MnPrv NaN 0 6 2010 WD Normal 105000 3 526351010 20 RL 81.0 14267 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN Gar2 12500 6 2010 WD Normal 172000 4 526353030 20 RL 93.0 11160 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 4 2010 WD Normal 244000 5 527105010 60 RL 74.0 13830 Pave NaN IR1 Lvl AllPub ... 0 NaN MnPrv NaN 0 3 2010 WD Normal 189900 <p>5 rows \u00d7 81 columns</p> In\u00a0[7]: Copied! <pre>raw_df.describe()\n</pre> raw_df.describe() Out[7]: PID MS SubClass Lot Frontage Lot Area Overall Qual Overall Cond Year Built Year Remod/Add Mas Vnr Area BsmtFin SF 1 ... Wood Deck SF Open Porch SF Enclosed Porch 3Ssn Porch Screen Porch Pool Area Misc Val Mo Sold Yr Sold SalePrice count 2.930000e+03 2930.000000 2440.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2907.000000 2929.000000 ... 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 2930.000000 mean 7.144645e+08 57.387372 69.224590 10147.921843 6.094881 5.563140 1971.356314 1984.266553 101.896801 442.629566 ... 93.751877 47.533447 23.011604 2.592491 16.002048 2.243345 50.635154 6.216041 2007.790444 180796.060068 std 1.887308e+08 42.638025 23.365335 7880.017759 1.411026 1.111537 30.245361 20.860286 179.112611 455.590839 ... 126.361562 67.483400 64.139059 25.141331 56.087370 35.597181 566.344288 2.714492 1.316613 79886.692357 min 5.263011e+08 20.000000 21.000000 1300.000000 1.000000 1.000000 1872.000000 1950.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 1.000000 2006.000000 12789.000000 25% 5.284770e+08 20.000000 58.000000 7440.250000 5.000000 5.000000 1954.000000 1965.000000 0.000000 0.000000 ... 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 0.000000 4.000000 2007.000000 129500.000000 50% 5.354536e+08 50.000000 68.000000 9436.500000 6.000000 5.000000 1973.000000 1993.000000 0.000000 370.000000 ... 0.000000 27.000000 0.000000 0.000000 0.000000 0.000000 0.000000 6.000000 2008.000000 160000.000000 75% 9.071811e+08 70.000000 80.000000 11555.250000 7.000000 6.000000 2001.000000 2004.000000 164.000000 734.000000 ... 168.000000 70.000000 0.000000 0.000000 0.000000 0.000000 0.000000 8.000000 2009.000000 213500.000000 max 1.007100e+09 190.000000 313.000000 215245.000000 10.000000 9.000000 2010.000000 2010.000000 1600.000000 5644.000000 ... 1424.000000 742.000000 1012.000000 508.000000 576.000000 800.000000 17000.000000 12.000000 2010.000000 755000.000000 <p>8 rows \u00d7 38 columns</p> In\u00a0[8]: Copied! <pre>raw_df.columns\n</pre>  raw_df.columns Out[8]: <pre>Index(['PID', 'MS SubClass', 'MS Zoning', 'Lot Frontage', 'Lot Area', 'Street',\n       'Alley', 'Lot Shape', 'Land Contour', 'Utilities', 'Lot Config',\n       'Land Slope', 'Neighborhood', 'Condition 1', 'Condition 2', 'Bldg Type',\n       'House Style', 'Overall Qual', 'Overall Cond', 'Year Built',\n       'Year Remod/Add', 'Roof Style', 'Roof Matl', 'Exterior 1st',\n       'Exterior 2nd', 'Mas Vnr Type', 'Mas Vnr Area', 'Exter Qual',\n       'Exter Cond', 'Foundation', 'Bsmt Qual', 'Bsmt Cond', 'Bsmt Exposure',\n       'BsmtFin Type 1', 'BsmtFin SF 1', 'BsmtFin Type 2', 'BsmtFin SF 2',\n       'Bsmt Unf SF', 'Total Bsmt SF', 'Heating', 'Heating QC', 'Central Air',\n       'Electrical', '1st Flr SF', '2nd Flr SF', 'Low Qual Fin SF',\n       'Gr Liv Area', 'Bsmt Full Bath', 'Bsmt Half Bath', 'Full Bath',\n       'Half Bath', 'Bedroom AbvGr', 'Kitchen AbvGr', 'Kitchen Qual',\n       'TotRms AbvGrd', 'Functional', 'Fireplaces', 'Fireplace Qu',\n       'Garage Type', 'Garage Yr Blt', 'Garage Finish', 'Garage Cars',\n       'Garage Area', 'Garage Qual', 'Garage Cond', 'Paved Drive',\n       'Wood Deck SF', 'Open Porch SF', 'Enclosed Porch', '3Ssn Porch',\n       'Screen Porch', 'Pool Area', 'Pool QC', 'Fence', 'Misc Feature',\n       'Misc Val', 'Mo Sold', 'Yr Sold', 'Sale Type', 'Sale Condition',\n       'SalePrice'],\n      dtype='object')</pre> In\u00a0[9]: Copied! <pre>import seaborn as sns\n\nsns.relplot(data=raw_df, x=\"Year Built\", y=\"SalePrice\")\n</pre>  import seaborn as sns  sns.relplot(data=raw_df, x=\"Year Built\", y=\"SalePrice\") Out[9]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x7f200554d2d0&gt;</pre> In\u00a0[10]: Copied! <pre>import plotly.express as px\nimport plotly.io as pio\n\npio.renderers.default = \"plotly_mimetype+sphinx_gallery\"\npx.scatter_3d(raw_df, x=\"Year Built\", y=\"SalePrice\", z=\"Lot Area\", color=\"House Style\")\n</pre>  import plotly.express as px import plotly.io as pio  pio.renderers.default = \"plotly_mimetype+sphinx_gallery\" px.scatter_3d(raw_df, x=\"Year Built\", y=\"SalePrice\", z=\"Lot Area\", color=\"House Style\") In\u00a0[11]: Copied! <pre>assert raw_df[\"PID\"].dtype == \"int64\"\nassert raw_df[\"Street\"].apply(lambda x: x in {\"Grvl\", \"Pave\"}).all()\n</pre> assert raw_df[\"PID\"].dtype == \"int64\" assert raw_df[\"Street\"].apply(lambda x: x in {\"Grvl\", \"Pave\"}).all()"},{"location":"reports/01_exploration/#1-raw-data-exploration","title":"1) Raw Data Exploration\u00b6","text":"<p>This shows how a Python file formatted with jupytext can be used to generate traceable reports</p>"},{"location":"reports/01_exploration/#development-setup-tips","title":"Development Setup Tips\u00b6","text":"<p>Automatically reload imported packages, so that you can develop in your python package and import any changes you made, without having to restart the kernel, see here.</p> <p>Since jupytext doesn't support the <code>%</code>-magic format, we have to call the magic slightly differently:</p>"},{"location":"reports/01_exploration/#data-exploration","title":"Data Exploration\u00b6","text":"<p>Here's how an initial data exploration could look like:</p>"},{"location":"reports/01_exploration/#data-validation","title":"Data Validation\u00b6","text":"<p>During the data exploration, it makes sense to formulate assumptions about the data, e.g. that there are no <code>NaN</code>s present, or that a certain column only contains value from a specific set.</p> <p>The simplest way to do this is via <code>assert</code> statements. If these assumptions are written in code, then this report will fail during execution and therefore warn you if in the future these assumptions don't hold true anymore.</p>"},{"location":"reports/01_exploration/#export-of-this-report","title":"Export of this report\u00b6","text":"<p>To export this report, simply use the \"Save Page\" or \"Print Page\" feature of your browser. Although some tools exist to export these pages, so far our exploration didn't find a better fitting solution than using your browser.</p>"},{"location":"reports/02_validation/","title":"2) Data Validation","text":"In\u00a0[1]: Copied! <pre>from integration_test_instance import constants, io\nfrom integration_test_instance.constants import PROJECT_ROOT\n\nprint(PROJECT_ROOT)\n</pre> from integration_test_instance import constants, io from integration_test_instance.constants import PROJECT_ROOT  print(PROJECT_ROOT) <pre>/__w/exploration-template-integration-test/exploration-template-integration-test\n</pre> In\u00a0[2]: Copied! <pre>raw_df = io.read_raw_data(constants.RAW_DATA_FILE)\n</pre> raw_df = io.read_raw_data(constants.RAW_DATA_FILE) In\u00a0[3]: Copied! <pre>assert raw_df[\"PID\"].dtype == \"int64\"\nassert raw_df[\"Street\"].apply(lambda x: x in {\"Grvl\", \"Pave\"}).all()\n</pre> assert raw_df[\"PID\"].dtype == \"int64\" assert raw_df[\"Street\"].apply(lambda x: x in {\"Grvl\", \"Pave\"}).all() In\u00a0[4]: Copied! <pre>from integration_test_instance.io.schemas import RawDataSchema\n\nRawDataSchema.validate(raw_df)\n</pre>  from integration_test_instance.io.schemas import RawDataSchema  RawDataSchema.validate(raw_df) Out[4]: PID MS SubClass MS Zoning Lot Frontage Lot Area Street Alley Lot Shape Land Contour Utilities ... Pool Area Pool QC Fence Misc Feature Misc Val Mo Sold Yr Sold Sale Type Sale Condition SalePrice Order 1 526301100 20 RL 141.0 31770 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN NaN 0 5 2010 WD Normal 215000 2 526350040 20 RH 80.0 11622 Pave NaN Reg Lvl AllPub ... 0 NaN MnPrv NaN 0 6 2010 WD Normal 105000 3 526351010 20 RL 81.0 14267 Pave NaN IR1 Lvl AllPub ... 0 NaN NaN Gar2 12500 6 2010 WD Normal 172000 4 526353030 20 RL 93.0 11160 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 4 2010 WD Normal 244000 5 527105010 60 RL 74.0 13830 Pave NaN IR1 Lvl AllPub ... 0 NaN MnPrv NaN 0 3 2010 WD Normal 189900 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 2926 923275080 80 RL 37.0 7937 Pave NaN IR1 Lvl AllPub ... 0 NaN GdPrv NaN 0 3 2006 WD Normal 142500 2927 923276100 20 RL NaN 8885 Pave NaN IR1 Low AllPub ... 0 NaN MnPrv NaN 0 6 2006 WD Normal 131000 2928 923400125 85 RL 62.0 10441 Pave NaN Reg Lvl AllPub ... 0 NaN MnPrv Shed 700 7 2006 WD Normal 132000 2929 924100070 20 RL 77.0 10010 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 4 2006 WD Normal 170000 2930 924151050 60 RL 74.0 9627 Pave NaN Reg Lvl AllPub ... 0 NaN NaN NaN 0 11 2006 WD Normal 188000 <p>2930 rows \u00d7 81 columns</p> <p>Instead of manually validating via <code>RawDataSchema.validate(...)</code>, pandera also offers decorators, which can be directly applied to the IO functions.</p> <p>See the reference for <code>integration_test_instance.io.read_raw_data_with_schema</code>:</p> In\u00a0[5]: Copied! <pre>from integration_test_instance.io import read_raw_data_with_schema\n\nraw_data_with_schema = read_raw_data_with_schema(constants.RAW_DATA_FILE)\n</pre> from integration_test_instance.io import read_raw_data_with_schema  raw_data_with_schema = read_raw_data_with_schema(constants.RAW_DATA_FILE) <p>The individual attributes of the schema have their docstrings attached:</p> <p></p> <p>Additionally, the schema documentation is part of the code reference.</p>"},{"location":"reports/02_validation/#2-data-validation","title":"2) Data Validation\u00b6","text":"<p>Any findings and assumptions formulated based on the data exploration should be encoded and validated automatically.</p> <p>This supports the MLOps principles in the following way:</p> <ul> <li>Decisions regarding data descriptions are auditable via Git</li> <li>Collaborating is easier due to the explicit documentation</li> <li>Assumptions formed during the early phases of a project are continuously validated</li> </ul> <p>This notebook shows two ways to do this:</p> <ul> <li>A) Via <code>assert</code> statements</li> <li>B) Via pandera</li> </ul>"},{"location":"reports/02_validation/#a-via-assert-statements","title":"A) Via <code>assert</code> statements\u00b6","text":"<p>The most barebones solution to validate assumptions about the data. This report will fail during execution time, if the assumptions don't hold true anymore</p>"},{"location":"reports/02_validation/#b-via-pandera","title":"B) Via pandera\u00b6","text":"<p>Pandera is a library designed both for schema validation (i.e., which columns are present and what datatype do they have), as well as value checking (are the values in an expected range, are <code>NaN</code>s present, do they have a minimum variance)</p>"},{"location":"reports/sub_dir/03_ml_libs/","title":"3) ML Libs","text":"In\u00a0[1]: Copied! <pre>try:\n    import tensorflow as tf\n\n    tf.__version__\nexcept ModuleNotFoundError:\n    print(\n        \"\"\"Tensorflow couldn't be imported, probably it wasn't selected as dependency\n    during the instantiation of the template.\"\"\"\n    )\n</pre> try:     import tensorflow as tf      tf.__version__ except ModuleNotFoundError:     print(         \"\"\"Tensorflow couldn't be imported, probably it wasn't selected as dependency     during the instantiation of the template.\"\"\"     ) <pre>2024-02-12 10:50:37.343931: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-02-12 10:50:37.378483: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n2024-02-12 10:50:37.379206: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n</pre> <pre>2024-02-12 10:50:39.619647: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n</pre> In\u00a0[2]: Copied! <pre>try:\n    import torch\n\n    torch.__version__\nexcept ModuleNotFoundError:\n    print(\n        \"\"\"Pytorch couldn't be imported, probably it wasn't selected as dependency\n    during the instantiation of the template.\"\"\"\n    )\n</pre> try:     import torch      torch.__version__ except ModuleNotFoundError:     print(         \"\"\"Pytorch couldn't be imported, probably it wasn't selected as dependency     during the instantiation of the template.\"\"\"     )"},{"location":"reports/sub_dir/03_ml_libs/#3-ml-libs","title":"3) ML Libs\u00b6","text":"<p>This notebook just verifies that the ML libs can be imported</p>"}]}